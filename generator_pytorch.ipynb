{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese Poetry Generator\n",
    "\n",
    "Generates Tang poetry using the CharRNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data: from https://github.com/chinese-poetry/chinese-poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from zhconv import convert\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import ParseRawData\n",
    "# This loads the json data and only takes the main body of each poem\n",
    "data_all = ParseRawData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57598"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 57598 Tang Poems in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'秦川雄帝宅，函谷壯皇居。綺殿千尋起，離宮百雉餘。連甍遙接漢，飛觀迥凌虛。雲日隱層闕，風煙出綺疎。'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter and prepare data:\n",
    "- convert traditional Chinese to simplified Chinese for better readability\n",
    "- only take poems with 5 character lines - easier for model to learn the poem structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30379"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comma = data_all[0][5]\n",
    "data = [convert(x, \"zh-hans\") for x in data_all if len(x.split(comma)[0])==5]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'秦川雄帝宅，函谷壮皇居。绮殿千寻起，离宫百雉馀。连甍遥接汉，飞观迥凌虚。云日隐层阙，风烟出绮疏。'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_counter = Counter([x for poem in data for x in poem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2607"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in char_counter if char_counter[x] < 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix = {}\n",
    "min_freq = 10 # a relatively large number is chosen to make the dataset smaller to fit my laptop\n",
    "# EOP_TOKEN = '$'\n",
    "for poem in data:\n",
    "    for char in poem:\n",
    "        # only take chars that appeared at least min_freq times\n",
    "        if (char not in char_to_ix) and (char_counter[char] >= min_freq):\n",
    "            char_to_ix[char] = len(char_to_ix)\n",
    "# char_to_ix['$'] = len(char_to_ix)\n",
    "# char_to_ix['<START>'] = len(char_to_ix)\n",
    "ix_to_char = dict((i, w) for (w, i) in char_to_ix.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 4442\n"
     ]
    }
   ],
   "source": [
    "# plus one to count for the unknown chars\n",
    "vocab_size = len(char_to_ix) + 1\n",
    "print(\"vocab size: {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1775989\n"
     ]
    }
   ],
   "source": [
    "# get input and target for training.\n",
    "# use first 6 chars as input to predict the next char.\n",
    "seq_len = 6\n",
    "input_data = []\n",
    "target = []\n",
    "# EOP_TOKEN = '$'\n",
    "for poem in data:\n",
    "    # add EOP token\n",
    "#     poem += EOP_TOKEN\n",
    "    for i in range(len(poem) - seq_len):\n",
    "        target.append(poem[i + seq_len])\n",
    "        input_data.append(poem[i:i+seq_len])\n",
    "\n",
    "print(\"Number of training samples: {}\".format(len(input_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'秦川雄帝宅，'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_X(text, char_to_ix):\n",
    "    \"\"\"Transforms one row of input text to onehot encoded tensors\n",
    "    input format: '秦川雄帝宅，'\n",
    "    output format: onehot encoded tensors\n",
    "    \"\"\"\n",
    "    X = np.zeros((len(text), len(char_to_ix)+1))\n",
    "    for i, char in enumerate(text):\n",
    "        X[i, char_to_ix.get(char, len(char_to_ix))] = 1\n",
    "    return X\n",
    "\n",
    "\n",
    "# def transform_y(text, char_to_ix):\n",
    "#     \"\"\"Transform the target text into onehot encoded tensor\"\"\"\n",
    "#     y = np.zeros((len(char_to_ix)+1))\n",
    "#     y[char_to_ix.get(text, len(char_to_ix))] = 1\n",
    "#     return y\n",
    "\n",
    "# For Pytorch crossentropyloss, the target does not need to be onehot encoded.\n",
    "def transform_y(text, char_to_ix):\n",
    "    return char_to_ix.get(text, len(char_to_ix))\n",
    "\n",
    "# dataset is too big for my laptop. Build a custom dataset\n",
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, data, target, transform_X, transform_y, char_to_ix):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.transform_X = transform_X\n",
    "        self.transform_y = transform_y\n",
    "        self.char_to_ix = char_to_ix\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Total number of samples\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one sample of data\"\"\"\n",
    "        X = self.transform_X(self.data[index], self.char_to_ix)\n",
    "        y = self.transform_y(self.target[index], self.char_to_ix)\n",
    "        sample = {\"input\": X, \"target\": y}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_dataset = PoemDataset(input_data, target, transform_X, transform_y, char_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "dataloader = DataLoader(poem_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check we have the right shape\n",
    "# for i, sample_batched in enumerate(dataloader):\n",
    "#     print(i, sample_batched['input'].size(),\n",
    "#          sample_batched['target'].size(),\n",
    "#          sample_batched['target'].dtype)\n",
    "#     if i == 3:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoemGenerationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(PoemGenerationModel, self).__init__()\n",
    "    \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(vocab_size, hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "#         embeds = self.embedding(input_data)\n",
    "        lstm_out, _ = self.lstm(input_data)\n",
    "        logits = self.linear(lstm_out[:, -1, :].squeeze())\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "hidden_dim = 256\n",
    "lr = 0.001\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = PoemGenerationModel(vocab_size, embed_dim, hidden_dim).float().to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoemGenerationModel(\n",
      "  (embedding): Embedding(4442, 128)\n",
      "  (lstm): LSTM(4442, 256, batch_first=True)\n",
      "  (linear): Linear(in_features=256, out_features=4442, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2615320972023661"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(0.2, 1/1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"Sample the output based predicted probabilities.\n",
    "    \n",
    "    preds: 1D tensor. Logits from the model\n",
    "    temperature: When temperature is low, tend to choose the most likely words. \n",
    "    When temperature is high, model will be more adventurous. \n",
    "    \"\"\"\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = torch.nn.functional.softmax(preds, dim=0).detach().cpu().numpy()\n",
    "    \n",
    "    exp_preds = np.power(preds, 1./temperature)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    pro = np.random.choice(range(len(preds)), 1, p=preds)\n",
    "    return int(pro.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem(input_text, output_length=18, temperature=1):\n",
    "    \"\"\"Given input_text, generate a poem.\n",
    "    input_text need to be 6 chars, where last one is a comma.\n",
    "    Example input: \"我有紫霞想，\"\n",
    "    \"\"\"\n",
    "    generated = \"\"\n",
    "    for i in range(output_length):\n",
    "        pred = generate_one_char(input_text, temperature=temperature)\n",
    "        generated += pred\n",
    "        input_text = input_text[1:] + pred\n",
    "    return generated\n",
    "    \n",
    "def generate_one_char(input_text, temperature=1):\n",
    "    X_test = np.zeros((1, seq_len, vocab_size))\n",
    "    for t, char in enumerate(input_text):\n",
    "        X_test[0, t, char_to_ix.get(char, len(char_to_ix))] = 1\n",
    "        \n",
    "    pred = model(torch.from_numpy(X_test).float()) #use less precision for laptop\n",
    "    next_index = sample(pred, temperature)\n",
    "    next_char = ix_to_char.get(next_index, \"?\")\n",
    "    \n",
    "    return next_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    if epoch % 2 == 0:\n",
    "        print()\n",
    "        print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "        # randomly pick the starting line of a poem as the seed\n",
    "        poem_index = random.randint(0, len(data))\n",
    "        print(\"Generating with seed: {}\".format(data[poem_index][:seq_len]))\n",
    "        seed_text = data[poem_index][:seq_len]\n",
    "        for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "            print('----- temperature:', temperature)\n",
    "            generated = generate_poem(seed_text, temperature=temperature)\n",
    "            print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33f66f802a847979360587eccbbed33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  Loss: 4.846212387084961\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "Generating with seed: 我有紫霞想，\n",
      "----- temperature: 0.2\n",
      "无因青草生。因君不可见，此去无所思。\n",
      "----- temperature: 0.5\n",
      "何人理白云。朝廷风云外，主人不可知。\n",
      "----- temperature: 1.0\n",
      "何时长守苏。谁敢苦轩蔡，百年亡洛浮。\n",
      "----- temperature: 1.2\n",
      "那忘名候违。疏并随衫蝶，四年落齿罗。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7bf4e042194626b29ef3d5c9bee991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  Loss: 5.119420528411865\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89fc4fcbe904c38b1187f70063e1abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-300-af8abe7669d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Compute loss and update gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-292-a8f1ab4dae55>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#         embeds = self.embedding(input_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#         predictions = self.softmax(linear_out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    582\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "#     model.train()\n",
    "  \n",
    "    # Dataloader returns the batches\n",
    "    for samples in tqdm(dataloader):\n",
    "        cur_batch_size = len(samples)\n",
    "        batch_X = samples['input'].to(device)\n",
    "        batch_y = samples['target'].to(device)\n",
    "\n",
    "        # Zero out the gradients before backpropagation\n",
    "        model.zero_grad()\n",
    "\n",
    "        y_pred = model(batch_X.float())\n",
    "        # Compute loss and update gradients\n",
    "        loss = loss_function(y_pred, batch_y.long())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "#         nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch {}:  Loss: {}\".format(epoch, loss))\n",
    "    on_epoch_end(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"saved_model/charrnn_pytorch_no_embedding\"\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the saved model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "挥灼朱丝折。路通祖窗云，面侧看钩嵩。\n"
     ]
    }
   ],
   "source": [
    "trained_model = PoemGenerationModel(vocab_size, embed_dim, hidden_dim)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "君为白日来。今来一何事，相见日相逢。\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"我有紫霞想，\"\n",
    "generated = generate_poem(seed_text, temperature=0.2)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我无白日来。君看一相见，谁与鬓毛斑。\n"
     ]
    }
   ],
   "source": [
    "generated = generate_poem(seed_text, temperature=0.3)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我有青山期。我有一生事，此时无所求。一朝一尊酒，一夜一尊酒。一日一尊酒，一朝一醉醒。\n"
     ]
    }
   ],
   "source": [
    "generated = generate_poem(seed_text, output_length=42, temperature=0.2)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "空堂青史书。回瞻汉武库，直入汉宫中。明月临江浦，清风满树枝。半空回首起，却忆故人来。\n"
     ]
    }
   ],
   "source": [
    "generated = generate_poem(\"明月几时有，\", output_length=42, temperature=0.5)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清风吹不闻。今来一何事，自有一生心。\n"
     ]
    }
   ],
   "source": [
    "generated = generate_poem(\"明月几时有，\", output_length=18, temperature=0.2)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    " - add one more lstm layer?\n",
    " - add embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
